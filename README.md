# Notes on LLM

## 1. Foundations

### 1.1. Word2Vec

[Efficient Estimation of Word Representations in Vector Space (2013)](https://drive.google.com/file/d/1jijzgFJh6xpJIaxJdUYpECONITKWaIOd/view?usp=drive_link)

[Distributed Representations of Words and Phrases and their Compositionality (2013)](https://drive.google.com/file/d/1ZodW3uPEj6j_x4H-ATcsygHfwBhjt7Ti/view?usp=drive_link)

[(YouTube) CS224N Lecture 2](https://www.youtube.com/watch?v=ERibwqs9p38)

### 1.2. Transformer

[Attention Is All You Need (2017)](https://drive.google.com/file/d/1DSC0LqzxVaqSoDpa_rEJRgkVVNhhXTAv/view?usp=drive_link) explained at [(YouTube) Pytorch Transformers from Scratch (Attention is all you need)](https://www.youtube.com/watch?v=U0s0f995w14)

### 1.3. GPT

GPT1 [Improving language understanding by generative pre-training (2018)](https://drive.google.com/file/d/1bJtZ6503tNZWwe3ZxNN-v5945SaY9l5b/view?usp=drive_link) explained at [(YouTube) Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)

### 1.3. Finetuning

Self-Instruct [Aligning Language Models with Self-Generated Instructions (2023)](https://drive.google.com/file/d/1b51GFMC3rRhIop0drRehjI0_xk8Hl3AW/view?usp=drive_link) in ChatGPT [Instruction tuning with gpt-4 (2023)](https://drive.google.com/file/d/1QD_N5mkCTyiSD7e4QWo2Sf5pdO9Dixph/view?usp=drive_link)

Others:
- [Visual Instruction Tuning (2023)](https://drive.google.com/file/d/1S3AiXcnpUpRNoLsPggv4r0FkpVOq9ZbW/view?usp=drive_link)
- Alpaca [Introduction](https://crfm.stanford.edu/2023/03/13/alpaca.html) [Git Repo](https://github.com/tatsu-lab/stanford_alpaca)

#### 2. Knowledge Distillation

In machine learning, knowledge distillation or model distillation is the process of transferring knowledge from a large model to a smaller one. (Wikipedia)

[Distilling the Knowledge in a Neural Network (2015)](https://drive.google.com/file/d/1_3s_dt0JdjF8uSBbAOgB_PnnSWdIAMWv/view?usp=drive_link) based on [Model Compression (2006)](https://drive.google.com/file/d/1FbOKVBU6assQ4HhOilz5jUbp8uMskRos/view?usp=drive_link) explained at [(YouTube) Distilling the Knowledge in a Neural Network](https://www.youtube.com/watch?v=k63qGsH1jLo)

In LLM [MiniLLM: Knowledge distillation of large language models (2023)](https://drive.google.com/file/d/15uv7ibazjsLrU_FBUDRfYLkwH6scPzrP/view?usp=drive_link)

Example Application [Knowledge Distillation of LLMs for Automatic Scoring of Science Assessments (2024)](https://drive.google.com/file/d/1pXCPfhzBiVIWOkBCr0DlqugbHM669Al1/view?usp=drive_link)

## 3. Knowledge Graph

In knowledge representation and reasoning, a knowledge graph is a knowledge base that uses a graph-structured data model or topology to represent and operate on data. (Wikipedia)

[Talk like a graph: Encoding graphs for large language models (2023)](https://drive.google.com/file/d/1hohBUb5tNbBL1_xzgJ9JsJrtJ6dr3fVk/view?usp=drive_link)

[LLM-assisted knowledge graph engineering: Experiments with chatgpt (2023)](https://drive.google.com/file/d/1AvqaiRoCr69XDk5BDc7EK4SDhPGf8hNc/view?usp=drive_link)

## 4. Retrieval-Augmented Generation

Retrieval augmented generation (RAG) is a type of generative artificial intelligence that has information retrieval capabilities. (Wikipedia)

Foundation [Retrieval-augmented generation for knowledge-intensive nlp tasks (2020)](https://drive.google.com/file/d/1cBHo9P7xPiIMzcH8C0MStmV15vWezgpJ/view?usp=drive_link), survey [Retrieval-augmented generation for ai-generated content: A survey (2024)](https://drive.google.com/file/d/1pYuH-L2WGC2dbtwYYsYI7gKGtturR1C7/view?usp=drive_link)

[Corrective Retrieval Augmented Generation (2024)](https://drive.google.com/file/d/1CffEMQMRx308YBicE6Cwlxe0d80ZaM9I/view?usp=drive_link)